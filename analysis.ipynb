{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# %config IPCompleter.greedy = True\n",
    "# %doctest_mode \n",
    "# %pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured.\n",
      "ValueError: Please install nodejs 5+ and npm before continuing installation. nodejs may be installed using conda or directly from the nodejs website.\n",
      "See the log file for details:  /tmp/jupyterlab-debug-dzzcv09v.log\n"
     ]
    }
   ],
   "source": [
    "# %pip install --user --upgrade nltk tldextract tqdm ipywidgets jupyterlab    \n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed96f70393714037a34eca5bd12a6c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ExtractResult(subdomain='', domain='bbc', suffix='co.uk')\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import doctest\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "import nltk\n",
    "import tldextract\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "for i in trange(5):\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "print(tldextract.extract(\"bbc.co.uk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"output/\")\n",
    "DATAPATH   = Path(\"ToAnalyse/\")\n",
    "GENRE_DICT = {\n",
    "\"Nation\": ['nation', 'nationalist', 'country', 'people', 'Australia', 'Australian', 'patriots', 'patriotic', 'aussies', 'flag', 'anthem', 'immigration', 'multiculturalism', 'ANZAC', 'military', 'soldiers' ],\n",
    "\"Islam\": ['muslim', 'Islam', 'Sharia', 'ISIS', 'mosque', 'Allah', 'Halal', 'Koran', 'infidel', 'akbar', 'burka', 'hijab', 'caliphate', 'jihad', 'jihadist', 'Islamisation', 'imem', 'mohammad', 'mecca', 'muzzie' ],\n",
    "\"Government and Politics\": ['government', 'parliament', 'council', 'councillor', 'policy', 'politics', 'turnbull', 'shorten', 'Andrews', 'Hanson' ],\n",
    "\"Racial References\": ['white', 'anglo', 'European', 'immigration', 'white genocide', 'replacement', 'great replacement', 'blacks', 'Africans', 'take over', 'invasion', 'breed', 'race', 'superior', '1488' ],\n",
    "\"Violence\": ['crimes', 'criminal', 'thugs', 'bashing', 'gang', 'probation', 'sentencing', 'parole', 'police', 'Cronulla', 'robbery', 'police', 'jail', 'prison', 'punishment', 'courts', 'victims', 'violence', 'Christchurch', 'Tarrant', 'Brevik', 'El Paso', 'shoot', 'shooting', 'mass-shooting'],\n",
    "\"Sexuality and Gender\": ['gender', 'transgender', 'sex', 'homosexual', 'sexual', 'paedophilia', 'gay', 'queer', 'lesbian', 'marriage', 'feminist', 'masculinity', 'sodomy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers')\n",
    "    print(\"Tokenizer found\")\n",
    "except:\n",
    "    print(\"Tokenizer not found\")\n",
    "    nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create folder in share â€œoutputâ€\n",
    "* For all other folders in the share:\n",
    "  * perform following analyses. ON each indvidual CSV AND ON all CSVs which exist in a subfolder of the current folder or within the current folder.\n",
    "    * Name of our output (called priorname below): â€œoutput/all involved folder names without spaces+name of csv if for individual csvâ€\n",
    "    * In priorname+â€domain_summary.csvâ€\n",
    "      * Extract all URLS from column labeled â€œFinal Linkâ€ if exists, else â€œLinkâ€\n",
    "        * domain, Count frequency of domain. \n",
    "    * In priorname+â€summary.csvâ€\n",
    "      * Find earliest creation date\n",
    "      * Find latest creation date\n",
    "      * Sum Likes Comments Shares Love Wow Haha Sad Angry Thankful, Post Views, Total Views\n",
    "      * Max Page Likes\n",
    "    * In priorname+â€activity_histogram.csvâ€\n",
    "      * columns: created (truncated to day), count of posts on that day\n",
    "    * Tokenise and lemmatise message+link text+description columns, \n",
    "      * Combine each tokenised message according to created date.\n",
    "      * Remove stopwords (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip) according to default english stopword list in NLTK (https://pythonspot.com/nltk-stop-words/)\n",
    "      * Create a lexical dispersion plot according to each of the following sets of words named priorname+â€œgenre.pngâ€\n",
    "      * Create a frequency list and output as priorname+frequency.csv\n",
    "        * In the frequency list, if a word appears in the genre list, label it in an appropriate column\n",
    "        * Columns: \n",
    "          * Lemmatised Word\n",
    "          * Count\n",
    "          * [Genres] (â€Trueâ€ if word appears in that genre) (Binary index)\n",
    "      * Generate a word cloud for the above tokens as per https://vprusso.github.io/blog/2018/natural-language-processing-python-3/ named priorname+â€wordcloud.pngâ€\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class analysis():\n",
    "    def __init__(self, original_file_name=None):\n",
    "        self.shared_domains_frequency = collections.OrderedDict()\n",
    "        self.nltk_tokens = []\n",
    "        self.max_page_likes = 0\n",
    "        self.post_count = 0\n",
    "        self.min_post_age = None\n",
    "        self.max_post_age = None\n",
    "        self.group = None\n",
    "        self.region = None\n",
    "        self.file = None\n",
    "        self.original_file_name = original_file_name\n",
    "        \n",
    "\n",
    "\n",
    "def csv_to_dict(file):\n",
    "    \"\"\"Imports a CSV into a format we can read\n",
    "    \n",
    "    >>> file = \"ToAnalyse/Australian Movement/Anti-Islam/2019-02-03-17-10-09-GMT-Historical-Report-Australian-Liberty-Alliance-1970-01-01--2019-02-03.csv\"\n",
    "    >>> csv_file = csv_to_dict(file)\n",
    "    >>> csv_file[0].keys()\n",
    "    odict_keys(['Page Name', 'User Name', 'Page Id', 'Page Likes at Posting', 'Created', 'Type', 'Likes', 'Comments', 'Shares', 'Love', 'Wow', 'Haha', 'Sad', 'Angry', 'Thankful', 'Video Share Status', 'Post Views', 'Total Views', 'Total Views for all Crossposts', 'URL', 'Message', 'Link', 'Final Link', 'Link Text', 'Description', 'Sponsor Id', 'Sponsor Name', 'Score'])\n",
    "    >>> len(csv_file)\n",
    "    2610\n",
    "    >>> csv_file[1]['Message'][:9]\n",
    "    'Bye Bye ðŸ£'\n",
    "    \"\"\"\n",
    "    \n",
    "    output = []\n",
    "    with open(file, \"r\", newline=\"\", encoding='utf-8-sig') as csvfile:\n",
    "        dictreader = csv.DictReader(csvfile)\n",
    "        for row in dictreader:\n",
    "            output.append(row)\n",
    "    return output\n",
    "    \n",
    "def extract_domain_count(csvdict):\n",
    "    \"\"\"In priorname+â€domain_summary.csvâ€\n",
    "        Extract all URLS from column labeled â€œFinal Linkâ€ if exists, else â€œLinkâ€\n",
    "        Cols: domain, Count frequency of domain.\n",
    "    \n",
    "    >>> file = \"ToAnalyse/Australian Movement/Anti-Islam/2019-02-03-17-10-09-GMT-Historical-Report-Australian-Liberty-Alliance-1970-01-01--2019-02-03.csv\"\n",
    "    >>> csv_file = csv_to_dict(file)\n",
    "    >>> domains = extract_domain_count(csv_file)\n",
    "\n",
    "    >>> domains['theaustralian.com.au']\n",
    "    118\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    links = []\n",
    "    domain_count = collections.OrderedDict()\n",
    "    for row in csvdict:\n",
    "\n",
    "        link = row.get('Final Link', row.get('Link', None))\n",
    "        if link:\n",
    "            tld = tldextract.extract(link)\n",
    "            links.append(\"{}.{}\".format(tld.domain, tld.suffix))\n",
    "    for link in links:\n",
    "        domain_count[link] = links.count(link)\n",
    "    #https://stackoverflow.com/a/613218/263449 \n",
    "    return collections.OrderedDict(sorted(domain_count.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "def add_ordered_dict(ordered_a, ordered_b, reverse=True):\n",
    "    \"\"\"Add two ordered dicts together\n",
    "    Remember, ordered dict are mutable\n",
    "    >>> a = collections.OrderedDict({'a': 5, 'b': 10})\n",
    "    >>> b = collections.OrderedDict({'c': 5, 'b': 10})\n",
    "    >>> c = collections.OrderedDict({'c': 5, 'a': 10})\n",
    "    \n",
    "    >>> add_ordered_dict(a,b)\n",
    "    OrderedDict([('b', 20), ('a', 5), ('c', 5)])\n",
    "    \n",
    "    >>> add_ordered_dict(c,add_ordered_dict(a,b))\n",
    "    OrderedDict([('b', 20), ('a', 15), ('c', 10)])\n",
    "    \n",
    "    >>> add_ordered_dict(c,add_ordered_dict(a,b), reverse=False)\n",
    "    OrderedDict([('c', 10), ('a', 15), ('b', 20)])\n",
    "    \"\"\"\n",
    "    output = collections.OrderedDict(ordered_a)\n",
    "    for row in ordered_b:\n",
    "        if row in ordered_a:\n",
    "            output[row] += ordered_b[row]\n",
    "        else:\n",
    "            output[row] = ordered_b[row]\n",
    "    \n",
    "    return collections.OrderedDict(sorted(output.items(), key=lambda kv: kv[1], reverse=reverse))\n",
    "    \n",
    "    \n",
    "def analyse_csv(file):\n",
    "    \"\"\"Run all analysis on a CSV\n",
    "    >>> file = PurePath(\"ToAnalyse/Australian Movement/Anti-Islam/2019-02-03-17-10-09-GMT-Historical-Report-Australian-Liberty-Alliance-1970-01-01--2019-02-03.csv\")\n",
    "    >>> output = analyse_csv(file)\n",
    "    >>> output.original_file_name\n",
    "    PurePosixPath('ToAnalyse/Australian Movement/Anti-Islam/2019-02-03-17-10-09-GMT-Historical-Report-Australian-Liberty-Alliance-1970-01-01--2019-02-03.csv')\n",
    "    \n",
    "    >>> output.file\n",
    "    '2019-02-03-17-10-09-GMT-Historical-Report-Australian-Liberty-Alliance-1970-01-01--2019-02-03.csv'\n",
    "    >>> output.group\n",
    "    'Anti-Islam'\n",
    "    \n",
    "    >>> output.region\n",
    "    'Australian Movement'\n",
    "    \n",
    "    >>> output.shared_domains_frequency['theaustralian.com.au']\n",
    "    118\n",
    "    \"\"\"\n",
    "\n",
    "    csv_file = csv_to_dict(file)\n",
    "    analysed_file = analysis(file)\n",
    "    \n",
    "    analysed_file.file = file.parts[-1]\n",
    "    analysed_file.group = file.parts[-2]\n",
    "    analysed_file.region = file.parts[-3]\n",
    "    \n",
    "    domains = extract_domain_count(csv_file)\n",
    "    analysed_file.shared_domains_frequency = domains\n",
    "    \n",
    "    return analysed_file\n",
    "doctest.testmod()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir exists, cleaning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8606e8647a7b4c77b3cc5e32189d9243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resetOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d79999e2c240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#os.remove(file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresetOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resetOutput' is not defined"
     ]
    }
   ],
   "source": [
    "# def resetOutput(output):\n",
    "#     shutil.rmtree(output, ignore_errors=True)\n",
    "#     os.makedirs(output)\n",
    "\n",
    "try:\n",
    "    os.makedirs(output)\n",
    "except:\n",
    "    print(\"Output dir exists, cleaning\")\n",
    "    for file in tqdm(OUTPUT_DIR.glob(\"**/*.csv\")):\n",
    "        print(file)\n",
    "        #os.remove(file)\n",
    "\n",
    "resetOutput(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_csv_data = []\n",
    "regions = {}\n",
    "\n",
    "for file in tqdm_notebook(DATAPATH.glob(\"**/*.csv\")):\n",
    "    purepath = PurePath(file)\n",
    "    \n",
    "    region = purepath.parts[-3]\n",
    "    group = purepath.parts[-2]\n",
    "    filename = purepath.parts[-1]\n",
    "    if region not in regions:\n",
    "        regions.update({region:[]})\n",
    "    if group not in regions[region]:\n",
    "        regions[region].append(group)\n",
    "    \n",
    "    #print(file, region, group, filename)\n",
    "    \n",
    "    individual_csv_data.append(analyse_csv(file))\n",
    "    \n",
    "print(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_domains = {}\n",
    "region_domains = {}\n",
    "\n",
    "def write_domains(filename, domains):\n",
    "    with open(OUTPUT_DIR / filename, \"w\", encoding=\"utf-8\", newline='') as outcsv:\n",
    "        csvwriter = csv.writer(outcsv)\n",
    "        csvwriter.writerow(['domain', 'frequency_of_mention'])\n",
    "        for row in domains:\n",
    "            csvwriter.writerow([row, domains[row]])\n",
    "            \n",
    "for individual_analysis in individual_csv_data:\n",
    "    filename = 'domains-{}-{}-{}'.format(individual_analysis.region, individual_analysis.group, individual_analysis.file)\n",
    "    write_domains(filename, individual_analysis.shared_domains_frequency)\n",
    "    \n",
    "    \n",
    "for region in regions:\n",
    "    print(region)\n",
    "    regiondomains = collections.OrderedDict()\n",
    "    for group in regions[region]:\n",
    "        print(\"\\t\", group)\n",
    "        groupdomains = collections.OrderedDict()\n",
    "        for individual_analysis in individual_csv_data:\n",
    "            if individual_analysis.group == group:\n",
    "                groupdomains = add_ordered_dict(groupdomains, individual_analysis.shared_domains_frequency)\n",
    "            if individual_analysis.region == region:\n",
    "                regiondomains = add_ordered_dict(regiondomains, individual_analysis.shared_domains_frequency)\n",
    "        filename = 'domains-{}-{}.csv'.format(region, group)\n",
    "        write_domains(filename, groupdomains)\n",
    "    filename = 'domains-{}.csv'.format(region)\n",
    "    write_domains(filename, regiondomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
